
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import gdown
import tempfile, os, io, csv, re
from pathlib import Path

st.set_page_config(page_title="æ¯æ—¥è‚¡ç¥¨ç¯©é¸å™¨ Â· GDrive ç‰ˆï¼ˆå¼·åŒ–å®¹éŒ¯ï¼‰", layout="wide")

# ----------------------------
# Helpers
# ----------------------------
def is_drive_file_url(s: str) -> bool:
    return "drive.google.com/file/d/" in s or "drive.google.com/uc?" in s

def extract_file_id(s: str) -> str:
    """å¾ file URL æˆ– ID å›å‚³ IDï¼›è‹¥å·²æ˜¯ ID ç›´æ¥å›å‚³"""
    if re.fullmatch(r"[A-Za-z0-9_\-]{20,}", s):
        return s
    m = re.search(r"/d/([A-Za-z0-9_\-]+)", s)
    if m: return m.group(1)
    m = re.search(r"[?&]id=([A-Za-z0-9_\-]+)", s)
    if m: return m.group(1)
    return ""

def direct_url_from_id(file_id: str) -> str:
    return f"https://drive.google.com/uc?export=download&id={file_id}"

@st.cache_data(ttl=3600, show_spinner="ä¸‹è¼‰è³‡æ–™ä¸­â€¦")
def download_file(input_text: str) -> str:
    """æ”¯æ´å¡«å…¥ FILE_ID æˆ–å®Œæ•´ file é€£çµï¼›ä¸‹è¼‰åˆ°æš«å­˜ï¼Œå›å‚³è·¯å¾‘"""
    file_id = extract_file_id(input_text) if not is_drive_file_url(input_text) else extract_file_id(input_text)
    if not file_id:
        raise RuntimeError("è¾¨è­˜ä¸åˆ° Google Drive æª”æ¡ˆ IDã€‚è«‹è²¼ã€æª”æ¡ˆåˆ†äº«é€£çµã€æˆ–ç›´æ¥è²¼ IDã€‚")
    url = direct_url_from_id(file_id)
    out_path = Path(tempfile.gettempdir()) / f"xq_{file_id}"
    # gdown æœƒè‡ªå‹•åŠ å‰¯æª”åï¼ˆè‹¥æœ‰ï¼‰ï¼Œå› æ­¤ç”¨ without suffix å…ˆæŒ‡å®šï¼Œå†ç”± gdown æ±ºå®š
    out = gdown.download(url, str(out_path), quiet=True, fuzzy=True)
    if out is None:
        raise RuntimeError("ä¸‹è¼‰å¤±æ•—ï¼ˆå¯èƒ½æ˜¯æ¬Šé™éã€çŸ¥é“é€£çµè€…å¯æª¢è¦–ã€ï¼Œæˆ– ID ä¸æ­£ç¢ºï¼‰ã€‚")
    p = Path(out)
    if not p.exists() or p.stat().st_size == 0:
        raise RuntimeError("ä¸‹è¼‰åˆ°çš„æª”æ¡ˆç‚ºç©ºã€‚è«‹ç¢ºèªæª”æ¡ˆæ¬Šé™èˆ‡å¤§å°ã€‚")
    return str(p)

def sniff_and_read_table(path: str) -> pd.DataFrame:
    """å˜—è©¦ CSVï¼ˆå«è‡ªå‹•åˆ†éš”ç¬¦ï¼‰â†’ XLSX å…©ç¨®è®€æ³•ï¼›ä¸¦ä½œæ›´æ˜ç¢ºéŒ¯èª¤è¨Šæ¯"""
    # å…ˆè©¦ CSV with sniffer
    with open(path, "rb") as f:
        head = f.read(4096)
    # å˜—è©¦åµæ¸¬æ˜¯å¦æ˜¯ZIPï¼ˆxlsxï¼‰
    if head.startswith(b"PK\x03\x04"):
        # Excel
        try:
            return pd.read_excel(path, engine="openpyxl")
        except Exception as e:
            raise RuntimeError(f"è®€å– Excel å¤±æ•—ï¼š{e}")

    # CSV è·¯å¾‘
    # å…ˆç”¨äºŒé€²ä½æ¢é ­ï¼Œå†é–‹æ–‡å­—ï¼ˆä¸ç¢ºå®šç·¨ç¢¼ï¼‰
    last_err = None
    encodings = ["cp950", "big5", "utf-8-sig", "utf-8"]
    for enc in encodings:
        try:
            # auto delimiter
            with open(path, "r", encoding=enc, errors="ignore") as f:
                sample = f.read(4096)
                try:
                    dialect = csv.Sniffer().sniff(sample, delimiters=",;\t|")
                    sep = dialect.delimiter
                except Exception:
                    # fallback: é€—è™Ÿ
                    sep = ","
            return pd.read_csv(path, encoding=enc, sep=sep, engine="python", low_memory=False)
        except Exception as e:
            last_err = e
    raise RuntimeError(f"è®€å– CSV å¤±æ•—ï¼ˆå¯èƒ½ä¸æ˜¯ CSV/XLSXï¼Œæˆ–å…§å®¹å—ä¿è­·ï¼‰ã€‚æœ€å¾ŒéŒ¯èª¤ï¼š{last_err}")

@st.cache_data(ttl=3600, show_spinner=False)
def load_data(input_text: str) -> pd.DataFrame:
    local_path = download_file(input_text)
    df = sniff_and_read_table(local_path)

    # æ—¥æœŸæ¬„ä½
    if "æ—¥æœŸ" not in df.columns:
        raise RuntimeError("ç¼ºå°‘ã€æ—¥æœŸã€æ¬„ä½ã€‚è«‹ç¢ºèªæª”æ¡ˆå«æœ‰å°è‚¡æ—¥æœŸï¼ˆYYYYMMDDï¼‰æ¬„ã€‚")
    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"].astype(str), format="%Y%m%d", errors="coerce")

    # ä»£ç¢¼/å•†å“æ¬„ä½
    if "ä»£ç¢¼" not in df.columns:
        raise RuntimeError("ç¼ºå°‘ã€ä»£ç¢¼ã€æ¬„ä½ã€‚")
    df["ä»£ç¢¼"] = df["ä»£ç¢¼"].astype(str)
    if "å•†å“" not in df.columns:
        df["å•†å“"] = ""

    # æ•¸å€¼æ¸…ç†
    def to_numeric(series):
        return pd.to_numeric(
            series.astype(str)
                  .str.replace(",", "", regex=False)
                  .str.replace("(", "-", regex=False)
                  .str.replace(")", "", regex=False),
            errors="coerce"
        )
    for c in [
        "é–‹ç›¤åƒ¹","æœ€é«˜åƒ¹","æœ€ä½åƒ¹","æ”¶ç›¤åƒ¹","æ¼²è·Œå¹…","æŒ¯å¹…","æˆäº¤é‡","å…§ç›¤é‡","å¤–ç›¤é‡","é–‹ç›¤é‡",
        "ç•¶æ—¥æ²–éŠ·å¼µæ•¸","52Håƒ¹","å‡åƒ¹","å‡åƒ¹[0+1]","å‡åƒ¹[1+2]","å‡åƒ¹[1+2+3]","å‡åƒ¹[0+1+2]",
        "èåˆ¸é¤˜é¡å¼µæ•¸","èåˆ¸å¢æ¸›å¼µæ•¸","æˆäº¤é‡‘é¡","é€±è½‰ç‡"
    ]:
        if c in df.columns:
            df[c] = to_numeric(df[c])

    return df.sort_values(["ä»£ç¢¼","æ—¥æœŸ"]).reset_index(drop=True)

def calc_abnormal_volume(df: pd.DataFrame, lookback: int = 5) -> pd.DataFrame:
    if "æˆäº¤é‡" not in df.columns:
        raise RuntimeError("ç¼ºå°‘ã€æˆäº¤é‡ã€æ¬„ä½")
    df = df.copy()
    df[f"å‡é‡{lookback}"] = (
        df.groupby("ä»£ç¢¼")["æˆäº¤é‡"]
          .transform(lambda s: s.shift(1).rolling(window=lookback, min_periods=max(1, lookback//2)).mean())
    )
    df["é‡èƒ½å€æ•¸"] = df["æˆäº¤é‡"] / df[f"å‡é‡{lookback}"]
    return df

def section_title(title: str, help_text: str = ""):
    cols = st.columns([1, 0.06])
    with cols[0]:
        st.subheader(title)
    with cols[1]:
        if help_text:
            st.caption(help_text)

# ----------------------------
# UI
# ----------------------------
st.sidebar.header("ğŸ“¦ è³‡æ–™ä¾†æºï¼ˆGoogle Drive æª”æ¡ˆé€£çµæˆ– IDï¼‰")
user_input = st.sidebar.text_input(
    "è²¼ä¸Šã€æª”æ¡ˆåˆ†äº«é€£çµã€æˆ–ç›´æ¥è²¼ FILE_ID",
    value="",
    help="ç¯„ä¾‹é€£çµï¼š https://drive.google.com/file/d/FILE_ID/view?usp=sharing"
)
if not user_input:
    st.info("è«‹åœ¨å·¦å´è²¼ä¸Šæª”æ¡ˆåˆ†äº«é€£çµæˆ– FILE_ID å¾Œé–‹å§‹ã€‚")
    st.stop()

# è¼‰å…¥
df = load_data(user_input)
df = calc_abnormal_volume(df, lookback=5)

st.sidebar.header("âš™ï¸ ç¯©é¸æ¢ä»¶")
py_dates = df["æ—¥æœŸ"].dropna().sort_values().dt.date.unique()
default_date_py = py_dates[-1] if len(py_dates) else None
sel_date = st.sidebar.date_input("é¸æ“‡æ—¥æœŸ", value=default_date_py)
if isinstance(sel_date, (list, tuple)):
    sel_date = sel_date[0] if len(sel_date) else default_date_py

top_n = st.sidebar.slider("Top Nï¼ˆèåˆ¸å¢æ¸› / é‡èƒ½ç•°å¸¸ï¼‰", 5, 50, 10, 5)
limit_up_threshold = st.sidebar.number_input("æ¼²åœé–€æª»ï¼ˆ%ï¼‰", 0.0, 20.0, 9.9, 0.1)
vol_lookback = st.sidebar.selectbox("é‡èƒ½å‡é‡è¦–çª—", options=[5, 10, 20], index=0)
vol_multiple = st.sidebar.number_input("é‡èƒ½å€æ•¸é–€æª»ï¼ˆâ‰¥ï¼‰", 1.0, 20.0, 2.0, 0.5)

df = calc_abnormal_volume(df, lookback=int(vol_lookback))
day = df.loc[df["æ—¥æœŸ"].dt.date == sel_date].copy()

# 1) æ¼²åœ
section_title("1ï¼‰æ¼²åœå€‹è‚¡", "æ¢ä»¶ï¼šæ¼²è·Œå¹… â‰¥ é–€æª»")
if "æ¼²è·Œå¹…" not in day.columns:
    st.warning("æ‰¾ä¸åˆ°ã€æ¼²è·Œå¹…ã€æ¬„ä½")
else:
    g = day.loc[day["æ¼²è·Œå¹…"] >= limit_up_threshold].copy()
    cols = ["ä»£ç¢¼","å•†å“","æ”¶ç›¤åƒ¹","æ¼²è·Œå¹…","æˆäº¤é‡","é€±è½‰ç‡"]
    st.dataframe(g[[c for c in cols if c in g.columns]].sort_values(["æ¼²è·Œå¹…","æˆäº¤é‡"], ascending=[False,False]), use_container_width=True)

# 2) èåˆ¸å¢æ¸›
section_title("2ï¼‰èåˆ¸å¢æ¸›æœ€å¤šå€‹è‚¡", "ä¾ã€èåˆ¸å¢æ¸›å¼µæ•¸ã€æ’åºï¼Œå– Top N")
if "èåˆ¸å¢æ¸›å¼µæ•¸" not in day.columns:
    st.warning("æ‰¾ä¸åˆ°ã€èåˆ¸å¢æ¸›å¼µæ•¸ã€æ¬„ä½")
else:
    cols = ["ä»£ç¢¼","å•†å“","æ”¶ç›¤åƒ¹","èåˆ¸å¢æ¸›å¼µæ•¸","èåˆ¸é¤˜é¡å¼µæ•¸","æˆäº¤é‡"]
    r = day.dropna(subset=["èåˆ¸å¢æ¸›å¼µæ•¸"]).sort_values("èåˆ¸å¢æ¸›å¼µæ•¸", ascending=False).head(top_n)
    st.dataframe(r[[c for c in cols if c in r.columns]], use_container_width=True)

# 3) é‡èƒ½ç•°å¸¸
section_title("3ï¼‰æˆäº¤é‡ç•°å¸¸å€‹è‚¡", "æ¢ä»¶ï¼šé‡èƒ½å€æ•¸ â‰¥ é–€æª»ï¼ˆä»Šæ—¥é‡ Ã· æ­·å²å‡é‡ï¼‰")
need_cols = {"æˆäº¤é‡", f"å‡é‡{vol_lookback}", "é‡èƒ½å€æ•¸"}
if not need_cols.issubset(set(day.columns)):
    miss = ", ".join(sorted(need_cols - set(day.columns)))
    st.warning(f"ç¼ºå°‘æ¬„ä½ï¼š{miss}")
else:
    cols = ["ä»£ç¢¼","å•†å“","æˆäº¤é‡",f"å‡é‡{vol_lookback}","é‡èƒ½å€æ•¸","æ”¶ç›¤åƒ¹","æ¼²è·Œå¹…"]
    v = day[(day["é‡èƒ½å€æ•¸"] >= float(vol_multiple)) & day[f"å‡é‡{vol_lookback}"].notna()].copy()
    v = v.sort_values("é‡èƒ½å€æ•¸", ascending=False).head(top_n)
    st.dataframe(v[[c for c in cols if c in v.columns]], use_container_width=True)

# å€‹è‚¡èµ°å‹¢
section_title("å€‹è‚¡èµ°å‹¢ï¼ˆäº’å‹•ï¼‰", "è¼¸å…¥ä»£ç¢¼")
codes_today = sorted(day["ä»£ç¢¼"].dropna().unique().tolist())
sel_code = st.text_input("è¼¸å…¥è‚¡ç¥¨ä»£ç¢¼", value=(codes_today[0] if codes_today else ""))

if sel_code:
    hist = df[df["ä»£ç¢¼"] == sel_code].copy()
    if hist.empty:
        st.info("æŸ¥ç„¡æ­¤ä»£ç¢¼çš„æ­·å²è³‡æ–™")
    else:
        st.markdown(f"**{sel_code} Â· {hist['å•†å“'].iloc[-1] if 'å•†å“' in hist.columns else ''}**")
        base = alt.Chart(hist).encode(x="æ—¥æœŸ:T")
        price_cols = [c for c in ["æ”¶ç›¤åƒ¹","é–‹ç›¤åƒ¹","æœ€é«˜åƒ¹","æœ€ä½åƒ¹"] if c in hist.columns]
        vol_cols = [c for c in ["æˆäº¤é‡", f"å‡é‡{vol_lookback}"] if c in hist.columns]

        if price_cols:
            st.altair_chart(
                base.mark_line().transform_fold(price_cols, as_=["æŒ‡æ¨™","æ•¸å€¼"]).encode(
                    y=alt.Y("æ•¸å€¼:Q", title="åƒ¹æ ¼"), color="æŒ‡æ¨™:N"
                ).properties(height=260),
                use_container_width=True
            )
        if vol_cols:
            st.altair_chart(
                base.transform_fold(vol_cols, as_=["é‡ç¨®","æ•¸å€¼"]).mark_bar().encode(
                    y=alt.Y("æ•¸å€¼:Q", title="æˆäº¤é‡ / å‡é‡"), color="é‡ç¨®:N"
                ).properties(height=180),
                use_container_width=True
            )

st.caption("è‹¥ä»å‡ºéŒ¯ï¼šç¢ºèªæª”æ¡ˆæ¬Šé™ï¼ˆä»»ä½•çŸ¥é“é€£çµè€…å¯æª¢è¦–ï¼‰ã€ç¢ºå®šæ˜¯ CSV æˆ– Excelï¼Œä¸¦åŒ…å«ã€æ—¥æœŸï¼ˆYYYYMMDDï¼‰ã€èˆ‡ã€ä»£ç¢¼ã€æ¬„ã€‚")
